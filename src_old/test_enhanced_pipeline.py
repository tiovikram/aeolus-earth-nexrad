#!/usr/bin/env python3
"""
Test script for the enhanced NEXRAD pipeline with AWS S3 data loading.
"""

import sys
import os
from datetime import datetime, timedelta
import logging

sys.path.append('.')
from enhanced_nexrad_pipeline import NexradDataLoader, NexradPipelineOptions
from apache_beam.options.pipeline_options import PipelineOptions

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def test_data_loader():
    """Test the NEXRAD data loader functionality."""
    
    logger.info("Testing NEXRAD Data Loader...")
    
    data_loader = NexradDataLoader()
    
    # Test 1: Get available stations for recent date
    test_date = datetime.now() - timedelta(days=1)
    logger.info(f"Testing station listing for {test_date.strftime('%Y-%m-%d')}")
    
    stations = data_loader.get_available_stations(test_date)
    logger.info(f"Found {len(stations)} stations")
    
    if stations:
        logger.info(f"Sample stations: {stations[:5]}")
        
        # Test 2: Find files for a station
        test_station = stations[0] if stations else 'KABR'
        start_time = test_date.replace(hour=12, minute=0, second=0, microsecond=0)
        end_time = start_time + timedelta(hours=2)
        
        logger.info(f"Testing file search for {test_station} from {start_time} to {end_time}")
        
        files = data_loader.find_files_for_station_and_timerange(
            test_station, start_time, end_time, max_files=3
        )
        
        logger.info(f"Found {len(files)} files for {test_station}")
        
        for file_info in files:
            logger.info(f"  - {file_info.filename} ({file_info.size:,} bytes) at {file_info.timestamp}")
        
        return len(files) > 0
    
    return False


def test_pipeline_options():
    """Test custom pipeline options parsing."""
    
    logger.info("Testing Pipeline Options...")
    
    # Test command line argument parsing
    test_args = [
        '--stations=KABR,KPDT',
        '--start_time=2024-01-01-12:00', 
        '--end_time=2024-01-01-18:00',
        '--output_path=./test_output',
        '--max_files_per_station=5',
        '--grid_size=80',
        '--include_sweeps=0,1,2',
        '--runner=DirectRunner'
    ]
    
    pipeline_options = PipelineOptions(test_args)
    nexrad_options = pipeline_options.view_as(NexradPipelineOptions)
    
    # Validate parsed options
    assert nexrad_options.stations == 'KABR,KPDT'
    assert nexrad_options.start_time == '2024-01-01-12:00'
    assert nexrad_options.end_time == '2024-01-01-18:00'
    assert nexrad_options.max_files_per_station == 5
    assert nexrad_options.grid_size == 80
    
    logger.info("‚úì Pipeline options parsing works correctly")
    return True


def test_small_pipeline_run():
    """Test a small pipeline run with minimal data."""
    
    logger.info("Testing Small Pipeline Run...")
    
    try:
        # Use recent time period to ensure data availability
        end_time = datetime.now() - timedelta(hours=1)  # 1 hour ago
        start_time = end_time - timedelta(minutes=30)   # 30 minute window
        
        # Create test arguments
        test_args = [
            '--stations=KABR',  # Single station to minimize processing
            f'--start_time={start_time.strftime("%Y-%m-%d-%H:%M")}',
            f'--end_time={end_time.strftime("%Y-%m-%d-%H:%M")}', 
            '--output_path=./test_enhanced_output',
            '--max_files_per_station=2',  # Very limited for testing
            '--grid_size=50',  # Small grid for speed
            '--include_sweeps=0',  # Single sweep only
            '--runner=DirectRunner',
            '--direct_running_mode=in_memory'  # Fastest for testing
        ]
        
        logger.info(f"Test parameters:")
        logger.info(f"  Station: KABR")
        logger.info(f"  Time window: {start_time} to {end_time}")
        logger.info(f"  Max files: 2")
        
        # First check if data is available
        from enhanced_nexrad_pipeline import create_file_list
        
        pipeline_options = PipelineOptions(test_args)
        nexrad_options = pipeline_options.view_as(NexradPipelineOptions)
        
        try:
            file_list = create_file_list(nexrad_options)
            logger.info(f"Found {len(file_list)} files for testing")
            
            if len(file_list) == 0:
                logger.warning("No files found for test period - this is normal")
                logger.info("Try running with a different time period or station")
                return True  # Not a failure - just no data
            
            # If we have data, we could run the pipeline here
            # For now, just validate that file discovery works
            logger.info("‚úì File discovery successful")
            logger.info("‚úì Pipeline setup successful")
            
            # Note: Actual pipeline execution would require:
            # from enhanced_nexrad_pipeline import process_nexrad_from_aws
            # process_nexrad_from_aws(nexrad_options)
            
            return True
            
        except ValueError as e:
            if "No NEXRAD files found" in str(e):
                logger.warning("No files found for test period - this is expected")
                return True
            else:
                raise
    
    except Exception as e:
        logger.error(f"Test pipeline run failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def run_all_tests():
    """Run all tests for the enhanced pipeline."""
    
    logger.info("=" * 60)
    logger.info("ENHANCED NEXRAD PIPELINE TEST SUITE")
    logger.info("=" * 60)
    
    tests = [
        ("Data Loader", test_data_loader),
        ("Pipeline Options", test_pipeline_options), 
        ("Small Pipeline Run", test_small_pipeline_run)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        logger.info(f"\n{'-' * 40}")
        logger.info(f"Running test: {test_name}")
        logger.info(f"{'-' * 40}")
        
        try:
            result = test_func()
            results.append((test_name, result))
            status = "‚úÖ PASSED" if result else "‚ùå FAILED"
            logger.info(f"Test {test_name}: {status}")
            
        except Exception as e:
            logger.error(f"Test {test_name} failed with exception: {e}")
            results.append((test_name, False))
    
    # Summary
    logger.info(f"\n{'=' * 60}")
    logger.info("TEST SUMMARY")
    logger.info(f"{'=' * 60}")
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for test_name, result in results:
        status = "‚úÖ PASSED" if result else "‚ùå FAILED"
        logger.info(f"{test_name:.<30} {status}")
    
    logger.info(f"\nOverall: {passed}/{total} tests passed")
    
    if passed == total:
        logger.info("üéâ All tests passed! Enhanced pipeline is ready to use.")
        logger.info("\nTry running:")
        logger.info("  python run_enhanced_pipeline.py --list-presets")
        logger.info("  python run_enhanced_pipeline.py --preset single_station_recent")
    else:
        logger.warning(f"‚ö†Ô∏è  {total - passed} test(s) failed")
    
    return passed == total


if __name__ == '__main__':
    success = run_all_tests()
    sys.exit(0 if success else 1)